{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.43.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.23-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy<3,>=1.23 (from streamlit)\n",
      "  Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from streamlit) (24.2)\n",
      "Collecting pandas<3,>=1.4.0 (from streamlit)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting requests<3,>=2.27 (from streamlit)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.23 (from llama-index)\n",
      "  Downloading llama_index_core-0.12.23.post2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.30.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading openai-1.65.5-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached SQLAlchemy-2.0.38-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached aiohttp-3.11.13-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.23->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pydantic>=2.8.0 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.14-py3-none-any.whl.metadata (902 bytes)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3,>=1.4.0->streamlit)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3,>=1.4.0->streamlit)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached aiohappyeyeballs-2.5.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached propcache-0.3.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.23.1-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.23->llama-index)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading streamlit-1.43.1-py2.py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.7 MB 799.2 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/9.7 MB 799.2 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 898.8 kB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 898.8 kB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 898.8 kB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 898.8 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 633.2 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 633.2 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 633.2 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 633.2 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 1.6/9.7 MB 544.8 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 1.8/9.7 MB 585.1 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 1.8/9.7 MB 585.1 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 1.8/9.7 MB 585.1 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.1/9.7 MB 578.5 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.1/9.7 MB 578.5 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.1/9.7 MB 578.5 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.1/9.7 MB 578.5 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.1/9.7 MB 578.5 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.1/9.7 MB 578.5 kB/s eta 0:00:14\n",
      "   --------- ------------------------------ 2.4/9.7 MB 471.0 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 2.6/9.7 MB 496.7 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 2.9/9.7 MB 529.3 kB/s eta 0:00:13\n",
      "   ------------ --------------------------- 3.1/9.7 MB 560.9 kB/s eta 0:00:12\n",
      "   --------------- ------------------------ 3.7/9.7 MB 632.1 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 3.9/9.7 MB 657.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.2/9.7 MB 682.0 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.5/9.7 MB 706.3 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.0/9.7 MB 755.0 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 5.2/9.7 MB 771.8 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 5.5/9.7 MB 793.3 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 5.8/9.7 MB 806.2 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.3/9.7 MB 849.9 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 6.6/9.7 MB 864.1 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 6.8/9.7 MB 877.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 895.7 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.6/9.7 MB 932.1 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 7.9/9.7 MB 942.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.1/9.7 MB 958.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/9.7 MB 981.5 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 8.9/9.7 MB 999.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.7 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.7 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading llama_index-0.12.23-py3-none-any.whl (7.0 kB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.12.23.post2-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.8/1.6 MB 745.8 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.8/1.6 MB 745.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.0/1.6 MB 729.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 828.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 946.8 kB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl (16 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.6-py3-none-any.whl (40 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.3 MB 1.9 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.3/25.3 MB 2.2 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/25.3 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/25.3 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/25.3 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.9/25.3 MB 2.0 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 3.4/25.3 MB 1.9 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 3.7/25.3 MB 1.9 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 3.7/25.3 MB 1.9 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 3.7/25.3 MB 1.9 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 3.9/25.3 MB 1.6 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 4.2/25.3 MB 1.5 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 4.5/25.3 MB 1.5 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 5.0/25.3 MB 1.5 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 5.2/25.3 MB 1.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 5.8/25.3 MB 1.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 6.0/25.3 MB 1.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 6.3/25.3 MB 1.6 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.6/25.3 MB 1.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.1/25.3 MB 1.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.3/25.3 MB 1.6 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 7.9/25.3 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 8.1/25.3 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 8.7/25.3 MB 1.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 8.9/25.3 MB 1.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 9.4/25.3 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 9.7/25.3 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 10.0/25.3 MB 1.6 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 10.2/25.3 MB 1.6 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 10.5/25.3 MB 1.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 10.7/25.3 MB 1.6 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 11.0/25.3 MB 1.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 11.3/25.3 MB 1.6 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 11.8/25.3 MB 1.6 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 12.1/25.3 MB 1.6 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 12.6/25.3 MB 1.6 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 12.8/25.3 MB 1.6 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 13.4/25.3 MB 1.6 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 13.6/25.3 MB 1.6 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 13.9/25.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 14.2/25.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 14.7/25.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 15.2/25.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 15.5/25.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 15.7/25.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 16.0/25.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 16.3/25.3 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 16.5/25.3 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 17.0/25.3 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 17.0/25.3 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 17.6/25.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 18.4/25.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 18.6/25.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 19.1/25.3 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 19.7/25.3 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 20.2/25.3 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.4/25.3 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 21.0/25.3 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 21.5/25.3 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 21.8/25.3 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.3/25.3 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/25.3 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.1/25.3 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.6/25.3 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/25.3 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.3 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.3 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 1.7 MB/s eta 0:00:00\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Using cached aiohttp-3.11.13-cp312-cp312-win_amd64.whl (438 kB)\n",
      "Using cached beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading llama_cloud-0.1.14-py3-none-any.whl (261 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
      "Downloading narwhals-1.30.0-py3-none-any.whl (313 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading openai-1.65.5-py3-none-any.whl (474 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached SQLAlchemy-2.0.38-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl (207 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading llama_cloud_services-0.6.5-py3-none-any.whl (28 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached propcache-0.3.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.23.1-cp312-cp312-win_amd64.whl (237 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: striprtf, pytz, filetype, dirtyjson, wrapt, watchdog, urllib3, tzdata, tqdm, toml, tenacity, soupsieve, sniffio, smmap, rpds-py, regex, PyYAML, python-dotenv, pypdf, pydantic-core, pyarrow, protobuf, propcache, pillow, numpy, networkx, narwhals, mypy-extensions, multidict, marshmallow, MarkupSafe, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, cachetools, blinker, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, referencing, pydantic, pandas, nltk, jinja2, httpcore, gitdb, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, pydeck, jsonschema-specifications, httpx, gitpython, dataclasses-json, aiohttp, openai, llama-index-core, llama-cloud, jsonschema, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, altair, streamlit, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 SQLAlchemy-2.0.38 aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 altair-5.5.0 annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 beautifulsoup4-4.13.3 blinker-1.9.0 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 distro-1.9.0 filetype-1.2.0 frozenlist-1.5.0 fsspec-2025.3.0 gitdb-4.0.12 gitpython-3.1.44 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jinja2-3.1.6 jiter-0.9.0 joblib-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 llama-cloud-0.1.14 llama-cloud-services-0.6.5 llama-index-0.12.23 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.23.post2 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.8 llama-index-llms-openai-0.3.25 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.6 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 marshmallow-3.26.1 multidict-6.1.0 mypy-extensions-1.0.0 narwhals-1.30.0 networkx-3.4.2 nltk-3.9.1 numpy-2.2.3 openai-1.65.5 pandas-2.2.3 pillow-11.1.0 propcache-0.3.0 protobuf-5.29.3 pyarrow-19.0.1 pydantic-2.10.6 pydantic-core-2.27.2 pydeck-0.9.1 pypdf-5.3.1 python-dotenv-1.0.1 pytz-2025.1 referencing-0.36.2 regex-2024.11.6 requests-2.32.3 rpds-py-0.23.1 smmap-5.0.2 sniffio-1.3.1 soupsieve-2.6 streamlit-1.43.1 striprtf-0.0.26 tenacity-9.0.0 tiktoken-0.9.0 toml-0.10.2 tqdm-4.67.1 typing-inspect-0.9.0 tzdata-2025.1 urllib3-2.3.0 watchdog-6.0.0 wrapt-1.17.2 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-ollama\n",
      "  Downloading llama_index_llms_ollama-0.5.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.4 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-llms-ollama) (0.12.23.post2)\n",
      "Collecting ollama>=0.4.3 (from llama-index-llms-ollama)\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.11.13)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.2.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.18.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (24.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\odunayo\\documents\\machine learning projects\\llm project\\local llm with streamlit\\venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.3.1)\n",
      "Downloading llama_index_llms_ollama-0.5.3-py3-none-any.whl (7.8 kB)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, llama-index-llms-ollama\n",
      "Successfully installed llama-index-llms-ollama-0.5.3 ollama-0.4.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import streamlit as st\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat history in session state\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.3\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(model, messages):\n",
    "    try:\n",
    "        llm = Ollama(model=model, request_timeout=120.0)\n",
    "        resp = llm.stream_chat(messages)\n",
    "        response = \"\"\n",
    "        response_placeholder = st.empty()\n",
    "        for r in resp:\n",
    "            response += r.delta\n",
    "            response_placeholder.write(response)\n",
    "        logging.info(f\"Model: {model}, Messages: {messages}, Response: {response}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during streaming: {str(e)}\")\n",
    "        raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit App Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"Chat with LLMS Models\")\n",
    "    logging.info(\"App started\")\n",
    "\n",
    "    model = st.sidebar.selectbox(\"Choose a model\", [\"llama3\", \"phi3\", \"mistral\"]) \n",
    "    logging.info(f\"Model selected: {model}\")\n",
    "\n",
    "    if prompt:= st.chat_input(\"Your question\"):\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        logging.info(f\"User input: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"Chat with LLMs Models\")  # Set the title of the Streamlit app\n",
    "    logging.info(\"App started\")  # Log that the app has started\n",
    "\n",
    "    # Sidebar for model selection\n",
    "    model = st.sidebar.selectbox(\"Choose a model\", [\"llama3\", \"phi3\", \"mistral\"])\n",
    "    logging.info(f\"Model selected: {model}\")\n",
    "\n",
    "    # Prompt for user input and save to chat history\n",
    "    if prompt := st.chat_input(\"Your question\"):\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        logging.info(f\"User input: {prompt}\")\n",
    "\n",
    "        # Display the user's query\n",
    "        for message in st.session_state.messages:\n",
    "            with st.chat_message(message[\"role\"]):\n",
    "                st.write(message[\"content\"])\n",
    "\n",
    "        # Generate a new response if the last message is not from the assistant\n",
    "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                start_time = time.time()  # Start timing the response generation\n",
    "                logging.info(\"Generating response\")\n",
    "\n",
    "                with st.spinner(\"Writing...\"):\n",
    "                    try:\n",
    "                        # Prepare messages for the LLM and stream the response\n",
    "                        messages = [ChatMessage(role=msg[\"role\"], content=msg[\"content\"]) for msg in st.session_state.messages]\n",
    "                        response_message = stream_chat(model, messages)\n",
    "                        duration = time.time() - start_time  # Calculate the duration\n",
    "                        response_message_with_duration = f\"{response_message}\\n\\nDuration: {duration:.2f} seconds\"\n",
    "                        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_message_with_duration})\n",
    "                        st.write(f\"Duration: {duration:.2f} seconds\")\n",
    "                        logging.info(f\"Response: {response_message}, Duration: {duration:.2f} s\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # Handle errors and display an error message\n",
    "                        st.session_state.messages.append({\"role\": \"assistant\", \"content\": str(e)})\n",
    "                        st.error(\"An error occurred while generating the response.\")\n",
    "                        logging.error(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
